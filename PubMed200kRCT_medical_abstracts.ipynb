{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sevendi Eldrige Rifki Poluan🔥🔥🔥\n",
    "### Descriptions: NLP Fundamental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Apr  6 08:00:32 2023       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 530.30.02              Driver Version: 530.30.02    CUDA Version: 12.1     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                  Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3090         On | 00000000:65:00.0 Off |                  N/A |\r\n",
      "| 53%   45C    P2              129W / 390W|  23446MiB / 24576MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "# working environment\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.0.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m161.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0m\n",
      "\u001b[?25hCollecting pytz>=2020.1\n",
      "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m675.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.8/dist-packages (from pandas) (2.8.2)\n",
      "Collecting tzdata>=2022.1\n",
      "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m670.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.2->pandas) (1.14.0)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "Successfully installed pandas-2.0.0 pytz-2023.3 tzdata-2023.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.8/9.8 MB\u001b[0m \u001b[31m172.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting joblib>=1.1.1\n",
      "  Downloading joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m216.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.8/dist-packages (from scikit-learn) (1.23.5)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.2.0 scikit-learn-1.2.2 threadpoolctl-3.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Import the necessary libraries</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Download the PubMed200kRCT_medical_abstracts dataset, which is a collection of around 200,000 labelled Randomized Controlled Trial (RCT) abstracts, from the paper titled [\"PubMed 200k RCT: a Dataset for Sequential Sentence Classification in Medical Abstracts\"](https://arxiv.org/abs/1710.06071) published in 2017. For more detailed information about the datasets, please refer to the original paper.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'pubmed-rct'...\n",
      "remote: Enumerating objects: 33, done.\u001b[K\n",
      "remote: Counting objects: 100% (8/8), done.\u001b[K\n",
      "remote: Compressing objects: 100% (3/3), done.\u001b[K\n",
      "remote: Total 33 (delta 5), reused 5 (delta 5), pack-reused 25\u001b[K\n",
      "Unpacking objects: 100% (33/33), done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PubMed_200k_RCT\r\n",
      "PubMed_200k_RCT_numbers_replaced_with_at_sign\r\n",
      "PubMed_20k_RCT\r\n",
      "PubMed_20k_RCT_numbers_replaced_with_at_sign\r\n",
      "README.md\r\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt  test.txt  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/train.zip\n",
      "  inflating: pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/train.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/train.zip -d pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dev.txt  test.txt  train.txt  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Load the dataset from txt files and read their contents.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data = open(\"pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/train.txt\", 'r').readlines()\n",
    "test_data = open(\"pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/test.txt\", 'r').readlines()\n",
    "val_data = open(\"pubmed-rct/PubMed_200k_RCT_numbers_replaced_with_at_sign/dev.txt\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['###24491034\\n',\n",
       " 'BACKGROUND\\tThe emergence of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .\\n',\n",
       " 'BACKGROUND\\tThis paper describes the design and evaluation of Positive Outlook , an online program aiming to enhance the self-management skills of gay men living with HIV .\\n']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Dataset Preparation: Separating Input Features and Target</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data):\n",
    "    all_data = []\n",
    "    one_abstract = ''\n",
    "    for line in data:\n",
    "        if line.startswith(\"###\"):\n",
    "            pass\n",
    "        elif line.isspace():\n",
    "            one_abstract = one_abstract.split(\"\\n\") \n",
    "            for i, text in enumerate(one_abstract):\n",
    "                dict_data = {}\n",
    "                split = text.split(\"\\t\")\n",
    "                if len(split) > 1: \n",
    "                    label = split[0]\n",
    "                    target = '' . join(split[1]).strip()\n",
    "                    dict_data['target'] = label\n",
    "                    dict_data['text'] = target\n",
    "                    dict_data['line_index'] = i + 1\n",
    "                    dict_data['total_lines'] = len(one_abstract) \n",
    "                    \n",
    "                    all_data.append(dict_data)\n",
    "                \n",
    "            one_abstract = '' \n",
    "        else:\n",
    "            one_abstract += line\n",
    "            \n",
    "    return all_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_raw = pd.DataFrame(prepare_data(train_data))\n",
    "test_raw = pd.DataFrame(prepare_data(test_data))\n",
    "val_raw = pd.DataFrame(prepare_data(val_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>text</th>\n",
       "      <th>line_index</th>\n",
       "      <th>total_lines</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>The emergence of HIV as a chronic condition me...</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BACKGROUND</td>\n",
       "      <td>This paper describes the design and evaluation...</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>This study is designed as a randomised control...</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>The intervention group will participate in the...</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>METHODS</td>\n",
       "      <td>The program is based on self-efficacy theory a...</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       target                                               text  line_index  \\\n",
       "0  BACKGROUND  The emergence of HIV as a chronic condition me...           1   \n",
       "1  BACKGROUND  This paper describes the design and evaluation...           2   \n",
       "2     METHODS  This study is designed as a randomised control...           3   \n",
       "3     METHODS  The intervention group will participate in the...           4   \n",
       "4     METHODS  The program is based on self-efficacy theory a...           5   \n",
       "\n",
       "   total_lines  \n",
       "0           12  \n",
       "1           12  \n",
       "2           12  \n",
       "3           12  \n",
       "4           12  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_raw, y_train_raw = train_raw.text, train_raw.target\n",
    "x_test_raw, y_test_raw = test_raw.text, test_raw.target\n",
    "x_val_raw, y_val_raw = val_raw.text, val_raw.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The emergence of HIV as a chronic condition means that people living with HIV are required to take more responsibility for the self-management of their condition , including making physical , emotional and social adjustments .',\n",
       " 'BACKGROUND')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_raw[0], y_train_raw[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Convert the label into integer values for encoding in preparation to be fed into the network.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_raw_en = encoder.transform(y_train_raw)\n",
    "y_test_raw_en = encoder.transform(y_test_raw)\n",
    "y_val_raw_en = encoder.transform(y_val_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 2, 2, 2, 2, 2, 2, 2, 1])"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_raw_en[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Convert the dataset into a tensor format.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = tf.data.Dataset.from_tensor_slices((x_train_raw[:1000], y_train_raw_en[:1000]))\n",
    "test = tf.data.Dataset.from_tensor_slices((x_test_raw, y_test_raw_en))\n",
    "val = tf.data.Dataset.from_tensor_slices((x_val_raw, y_val_raw_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>\n",
    "    Configure the token count and sequence length for text vectorization and embedding.\n",
    "</font>\n",
    "</br>\n",
    "    \n",
    "    APPROXIMATE_TOKEN_COUNT: I calculated the approximate token count based on the number of unique tokens in the entire training set.\n",
    "\n",
    "    SEQ_LENGTH: I determined the sequence length based on the occurrence of tokens in the dataset, where I selected the length that covers approximately 97% of the tokens in the dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "APPROXIMATE_TOKEN_COUNT = len(set(' ' . join(x_train_raw).split(' ')))\n",
    "SEQ_LENGTH = int(np.percentile([len(x) for x in x_train_raw], 97))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(326178, 314)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "APPROXIMATE_TOKEN_COUNT, SEQ_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>We will set up the vectorization and embedding model to process the input sentences, focusing on **text-level features**.</font>\n",
    "    \n",
    "    The dataset adaptation process may take some time due to the large amount of data. To address this, you can consider using transfer learning by extracting pre-trained embeddings from existing models, which can provide better features and save time as well. But, in this case, we will not be using that approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vectorize = tf.keras.layers.TextVectorization(max_tokens=APPROXIMATE_TOKEN_COUNT,\n",
    "                                              output_mode=\"int\",\n",
    "                                              output_sequence_length=SEQ_LENGTH,\n",
    "                                              split=\"whitespace\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    }
   ],
   "source": [
    "start_time = time.perf_counter()\n",
    "vectorize.adapt(x_train_raw)\n",
    "end_time = time.perf_counter()\n",
    "print(end_time - start_time, 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 314), dtype=int64, numpy=\n",
       "array([[   2, 3009,    3,  545,   25,    8,  241,  559,  970,   26,  656,\n",
       "        1234,    7,  545,   67,  408,    6, 2116,   59, 9855,   12,    2,\n",
       "        2288,    3,  123,  559,  309, 2220,  239, 1528,    4,  642, 4723,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0]])>"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize([x_train_raw[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = tf.keras.layers.Embedding(input_dim=APPROXIMATE_TOKEN_COUNT, \n",
    "                                      output_dim=256, \n",
    "                                      input_length=SEQ_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 314, 256), dtype=float32, numpy=\n",
       "array([[[ 0.03627015, -0.03426458, -0.02339865, ...,  0.01825185,\n",
       "          0.01053284,  0.03210547],\n",
       "        [-0.04784089, -0.00884818, -0.02709348, ...,  0.01528518,\n",
       "          0.00518012, -0.02204286],\n",
       "        [ 0.01548724,  0.02079786,  0.0313834 , ..., -0.02326493,\n",
       "         -0.00597023,  0.03959525],\n",
       "        ...,\n",
       "        [-0.00377882, -0.01390042,  0.00276034, ...,  0.04459765,\n",
       "         -0.0454565 ,  0.03515568],\n",
       "        [-0.00377882, -0.01390042,  0.00276034, ...,  0.04459765,\n",
       "         -0.0454565 ,  0.03515568],\n",
       "        [-0.00377882, -0.01390042,  0.00276034, ...,  0.04459765,\n",
       "         -0.0454565 ,  0.03515568]]], dtype=float32)>"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(vectorize([x_train_raw[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>It's time to develop the model. In this case, I will be using a simple model for training.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_text\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_27 (InputLayer)       [(None, 1)]               0         \n",
      "                                                                 \n",
      " text_vectorization_7 (TextV  (None, 314)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 314, 256)          83501568  \n",
      "                                                                 \n",
      " first_conf (Conv1D)         (None, 313, 256)          131328    \n",
      "                                                                 \n",
      " second_conf (Conv1D)        (None, 311, 256)          196864    \n",
      "                                                                 \n",
      " third_conv (Conv1D)         (None, 309, 256)          196864    \n",
      "                                                                 \n",
      " g_pool (GlobalAveragePoolin  (None, 256)              0         \n",
      " g1D)                                                            \n",
      "                                                                 \n",
      " dense_bro (Dense)           (None, 128)               32896     \n",
      "                                                                 \n",
      " final_dense (Dense)         (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 84,060,165\n",
      "Trainable params: 84,060,165\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=(1,), dtype=tf.string)\n",
    "x = vectorize(inputs)\n",
    "x = embedding(x)\n",
    "x = tf.keras.layers.Conv1D(filters=256, kernel_size=2, activation=\"relu\", name=\"first_conf\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=256, kernel_size=3, activation=\"relu\", name=\"second_conf\")(x)\n",
    "x = tf.keras.layers.Conv1D(filters=256, kernel_size=3, padding=\"valid\", activation=\"relu\", name=\"third_conv\")(x)\n",
    "x = tf.keras.layers.GlobalAveragePooling1D(name=\"g_pool\")(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\", name=\"dense_bro\")(x)\n",
    "outputs = tf.keras.layers.Dense(len(set(y_train_raw)), activation=\"softmax\", name=\"final_dense\")(x)\n",
    "model_text = tf.keras.Model(inputs, outputs, name=\"model_text\")\n",
    "model_text.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Define callbacks for updating the learning rate, early stopping if the model does not show improvement, and saving the model if any improvement occurs.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = tf.keras.callbacks.ReduceLROnPlateau(monitor=\"loss\", # quantity to be monitored.\n",
    "                                                   factor=0.2, # factor by which the learning rate will be reduced. new_lr = lr * factor.\n",
    "                                                   patience=5, # number of epochs with no improvement after which learning rate will be reduced.\n",
    "                                                   min_lr=1e-7) # lower bound on the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",\n",
    "                                               patience=5, # Number of epochs with no improvement after which training will be stopped.\n",
    "                                               start_from_epoch=5,\n",
    "                                               mode=\"min\") # training will stop when the quantity monitored has stopped decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"checkpoint/model-1/model_text.hdf5\",\n",
    "                                                   monitor=\"val_loss\",\n",
    "                                                   save_best_only=True,\n",
    "                                                   save_weights_only=False,\n",
    "                                                   save_format='tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Define the metrics to be displayed during the training process.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='red'>Now let's proceed with training the model.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "\n",
    "train = train.batch(BATCH_SIZE).cache()\n",
    "test = test.batch(BATCH_SIZE).cache()\n",
    "val = val.batch(BATCH_SIZE).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_text.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                   optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
    "                   metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 08:45:22.199342: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1000]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-06 08:45:22.199855: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [1000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization_7' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n    \n    Call arguments received by layer 'text_vectorization_7' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, None), dtype=string)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[228], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m history_text \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m200\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m                              \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_file0k66shkk.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1284, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1268, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1249, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1050, in train_step\n        y_pred = self(x, training=True)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/usr/local/lib/python3.8/dist-packages/keras/layers/preprocessing/text_vectorization.py\", line 573, in _preprocess\n        raise ValueError(\n\n    ValueError: Exception encountered when calling layer 'text_vectorization_7' (type TextVectorization).\n    \n    When using `TextVectorization` to tokenize strings, the input rank must be 1 or the last shape dimension must be 1. Received: inputs.shape=(None, None) with rank=2\n    \n    Call arguments received by layer 'text_vectorization_7' (type TextVectorization):\n      • inputs=tf.Tensor(shape=(None, None), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "history_text = model_text.fit(train,\n",
    "                              epochs=200,\n",
    "                              validation_data=val,\n",
    "                              validation_steps=len(val),\n",
    "                              callbacks=[lr_callback, es_callback, save_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(32,), dtype=string, numpy=\n",
      "array([b'Adrenergic activation is thought to be an important determinant of outcome in subjects with chronic heart failure ( CHF ) , but baseline or serial changes in adrenergic activity have not been previously investigated in a large patient sample treated with a powerful antiadrenergic agent .',\n",
      "       b'Systemic venous norepinephrine was measured at baseline , @ months , and @ months in the beta-Blocker Evaluation of Survival Trial ( BEST ) , which compared placebo treatment with the beta-blocker/sympatholytic agent bucindolol .',\n",
      "       b'Baseline norepinephrine level was associated with a progressive increase in rates of death or death plus CHF hospitalization that was independent of treatment group .',\n",
      "       b'On multivariate analysis , baseline norepinephrine was also a highly significant ( P < @ ) independent predictor of death .',\n",
      "       b'In contrast , the relation of the change in norepinephrine at @ months to subsequent clinical outcomes was complex and treatment group-dependent .',\n",
      "       b'In the placebo-treated group but not in the bucindolol-treated group , marked norepinephrine increase at @ months was associated with increased subsequent risks of death or death plus CHF hospitalization .',\n",
      "       b'In the bucindolol-treated group but not in the placebo-treated group , the @st quartile of marked norepinephrine reduction was associated with an increased mortality risk .',\n",
      "       b'A likelihood-based method indicated that @ % of the bucindolol group but only @ % of the placebo group were at an increased risk for death related to marked reduction in norepinephrine at @ months .',\n",
      "       b'In BEST , a subset of patients treated with bucindolol had an increased risk of death as the result of sympatholysis , which compromised the efficacy of this third-generation beta-blocker .',\n",
      "       b'The study evaluated the effectiveness of a three-year outpatient commitment pilot program established in @ at Bellevue Hospital in New York City .',\n",
      "       b'A total of @ participants were randomly assigned ; @ received court-ordered treatment , which included enhanced services , and @ received the enhanced-service package only .',\n",
      "       b'Between @ and @ percent of the subjects completed interviews at one , five , and @ months after hospital discharge .',\n",
      "       b'Outcome measures included rehospitalization , arrest , quality of life , symptomatology , treatment noncompliance , and perceived level of coercion .',\n",
      "       b'On all major outcome measures , no statistically significant differences were found between the two groups .',\n",
      "       b'No subject was arrested for a violent crime .',\n",
      "       b'Eighteen percent of the court-ordered group and @ percent of the control group were arrested at least once .',\n",
      "       b'The percentage rehospitalized during follow-up was about the same for both groups-@ percent and @ percent , respectively .',\n",
      "       b'The groups did not differ significantly in the total number of days hospitalized during the follow-up period .',\n",
      "       b\"Participants ' perceptions of their quality of life and level of coercion were about the same .\",\n",
      "       b\"From the community service providers ' perspective , patients in the two groups were similarly adherent to their required treatments .\",\n",
      "       b'All results must be qualified by the fact that no pick-up order procedures for noncompliant subjects in the court-ordered group were implemented during the study , which compromised the differences between the conditions for the two groups , and that persons with a history of violence were excluded from the program .',\n",
      "       b\"Octylcyanoacrylate ( Dermabond ) is a tissue `` glue '' useful in closing surgical skin incisions .\",\n",
      "       b'We compared skin octylcyanoacrylate with subcuticular skin sutures to close laparoscopic trocar sites .',\n",
      "       b'A randomized double-armed study was performed with @ patients in whom @ trocar sites were closed .',\n",
      "       b'Twenty-nine patients underwent subcuticular closure with @-@ absorbable suture , and thirty patients received closure with octylcyanoacrylate in accordance with the recommendations of the manufacturer ( Ethicon ; Somerville , NJ ) .',\n",
      "       b'The number of sutures or vials of octylcyanoacrylate used , closure times , and postoperative wound problems were recorded .',\n",
      "       b'Wounds were assessed @ weeks postoperatively for healing complications .',\n",
      "       b'Closure costs were estimated using published operating room time per hour plus the cost of octylcyanoacrylate or suture .',\n",
      "       b\"Student 's paired t-test was used for statistical analysis .\",\n",
      "       b'The overall mean time required for skin closure using octylcyanoacrylate and suture was @ minutes and @ minutes , respectively ( P < @ ) .',\n",
      "       b'An average of @ packets of suture were used to close all port sites in a particular patient , while closure with octylcyanoacrylate required an average of @ vials per patient .',\n",
      "       b'Wound complications consisted of subcuticular seroma with skin separation and were equally common in the two groups .'],\n",
      "      dtype=object)>, <tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
      "array([0, 4, 4, 4, 4, 4, 4, 4, 1, 3, 2, 2, 2, 4, 4, 4, 4, 4, 4, 4, 1, 3,\n",
      "       3, 2, 2, 2, 2, 2, 2, 4, 4, 4])>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 08:43:25.016013: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [28932]\n",
      "\t [[{{node Placeholder/_1}}]]\n",
      "2023-04-06 08:43:25.024773: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "for x in val:\n",
    "    print(x)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
